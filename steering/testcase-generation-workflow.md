---
inclusion: manual
---

# 测试用例生成工作流

## 完整工作流程（分阶段，支持跨 session 恢复）

### 阶段0: 环境检查与任务决策（每次启动必做）

1. 调用 `setup_environment` 执行启动检查
2. 工具自动完成：依赖检查 → 目录创建 → 缓存任务检测
3. 根据返回结果决定后续流程：

**如果 `has_cache=true`（检测到缓存任务）：**
- 向用户展示缓存任务详情（阶段、图片进度、用例数量）
- 询问用户选择：
  - "继续上次任务" → 调用 `get_workflow_state` 恢复进度，按 resume 指令继续
  - "开始新任务" → 调用 `clear_cache` 清除缓存，进入阶段1

**如果 `has_cache=false`（无缓存）：**
- 直接进入阶段1

### 阶段1: 确认文档与选择处理模式（新任务时）

#### 1.1 确认文档就位
向用户确认文档已放入 `doc/` 目录：
- 需求文档（.docx）
- 概要设计文档（.docx）

等待用户明确确认后再继续。

#### 文档打标分类规则
系统根据文件名自动分类文档角色：
- 文件名含 `【主prd】` → 主需求文档
- 文件名含 `【主概设】` → 主概要设计文档
- 文件名含 `【主后端概设】` → 主后端概要设计文档
- 文件名含 `【主前端概设】` → 主前端概要设计文档
- 其他文件 → 辅助资料（关联参考文档）

主文档：提取图片 + 作为用例生成的目标文档
辅助资料：仅解析文字内容，不处理图片，仅在用例设计需要补充信息时按需查阅

如果所有文档都没有上述标签，则全部视为主文档（向后兼容）。

#### 1.2 选择处理模式
向用户说明两种模式：

| 模式 | 说明 | tokens 消耗 |
|------|------|-------------|
| 文档+图片模式 | 解析文字+图片，图片通过外部多模态LLM API分析，支持多线程 | 高（使用外部API） |
| 纯文档模式 | 仅解析文字内容，跳过图片分析 | 低 |

- 用户选择"文档+图片模式" → 执行阶段2，然后调用 `configure_llm_api` 打开GUI配置外部API，配置完成后调用 `process_images_with_llm` 批量处理图片
- 用户选择"纯文档模式" → 执行阶段2，跳过阶段2.5，直接进入阶段3

### 阶段2: 文档转换
1. 使用 `parse_documents` 将工作区中所有 `.docx` 文件转换为 Markdown
   - Markdown 文件输出到 `.tmp/doc_mk/` 目录
   - 图片提取到 `.tmp/picture/` 目录
   - 状态缓存到 `.tmp/cache/` 目录

### 阶段2.5: 图片处理（仅"文档+图片模式"）

1. 调用 `configure_llm_api` 打开GUI配置窗口
   - 输入API地址（如 http://localhost:4141/）
   - 输入API Key（非必填）
   - 点击"测试连接"验证连通性
   - 点击"获取模型列表"获取可用模型
   - 选择支持视觉的模型（如 gpt-4o）
   - 可选：启用多线程并发处理，设置线程数
   - 点击"确认并保存"
   - 配置会自动保存，下次打开时恢复上次输入
2. 调用 `process_images_with_llm` 批量处理所有待处理图片
   - 支持多线程并发（如已在GUI中启用）
   - 自动将分析结果回填到Markdown文档对应位置
   - 如果图片因清晰度不足导致模型无法准确解析，该图片会被自动跳过，不影响后续流程
   - 处理进度自动持久化，支持断点续传

### 阶段3: 测试用例生成（建议新 session）
图片处理完成后，建议开启新 session 进行用例生成，以获得最大上下文空间。

1. 调用 `setup_environment` → 检测到缓存 → 用户选择"继续" → 调用 `get_workflow_state` 恢复进度
2. 调用 `get_doc_summary` 获取文档结构概览（标题树 + 字数统计），文档按主文档/辅助资料分类显示
3. 优先按主文档的模块分批生成用例：
   a. 调用 `get_doc_section(doc_name, section_heading)` 读取主文档中一个模块的内容
   b. 基于该模块内容生成测试用例
   c. 如果用例设计中需要补充信息（如关联的数据结构定义、接口规范等），再按需从辅助资料中调用 `get_doc_section` 查阅对应章节
   d. 调用 `save_testcases(append_module=该模块的完整JSON对象)` 增量保存该模块用例
      - **必须传递 `append_module` 参数**（单个模块对象，非数组），不能省略
      - 示例：`save_testcases(append_module={"name":"模块名","sub_modules":[...]})`
   e. 重复处理下一个模块
4. 所有模块追加完毕后，调用 `get_testcases` 确认用例完整性

**避免使用 `get_parsed_markdown` 一次性加载全部文档内容！**
优先使用 `get_doc_summary` + `get_doc_section` 分段读取。

用例结构：
```json
{
  "modules": [
    {
      "name": "模块名称",
      "sub_modules": [
        {
          "name": "子模块名称",
          "test_cases": [
            {
              "title": "用例标题",
              "preconditions": "前置条件",
              "steps": ["步骤1", "步骤2"],
              "expected_result": "预期结果"
            }
          ]
        }
      ]
    }
  ]
}
```

XMind 导出层级（链式嵌套，每层单子节点）：
```
测试用例 (root)
  └─ 模块
      └─ 子模块
          └─ 用例标题
              └─ 前置条件: xxx
                  └─ 执行步骤: 1. xxx \n 2. xxx
                      └─ 预期结果: xxx
```

生成维度：
- 正向功能: 核心业务流程的正常执行
- 边界条件: 最大值、最小值、空值、特殊字符
- 异常处理: 网络异常、超时、权限不足、数据不存在
- 安全性: 认证、授权、数据加密、SQL注入、XSS
- 性能: 并发、大数据量、响应时间
- 兼容性: 不同设备、不同版本、不同状态

### 阶段3.5: 模块结构审查
用例初步生成完毕后，进入模块结构审查：

1. 调用 `review_module_structure` 获取模块结构审查报告
2. 根据报告中的问题和建议，调整模块划分：
   - 过大的模块（>15个用例的子模块）按场景拆分
   - 过小的模块（只有1个用例的子模块过多时）合并相关子模块
   - 修正空模块、重复命名等问题
   - 补充缺失的前置条件、步骤、预期结果
3. 调整后对需要调整的模块逐个调用 `save_testcases(append_module=调整后的单个模块对象)` 保存
4. 可再次调用 `review_module_structure` 确认调整效果

### 阶段4: 自动Review
对生成的用例进行自我审查：

1. 调用 `get_testcases` 获取当前所有用例
2. 按以下检查清单逐项审查：
   - 覆盖完整性: 文档中提到的每个功能点是否都有对应用例？
   - 图片覆盖: 流程图中的每条路径、状态图中的每个转换是否都有用例？
   - 边界充分性: 每个输入字段是否考虑了边界值？
   - 异常覆盖: 每个操作是否考虑了失败场景？
   - 步骤可执行性: 每个步骤是否清晰、可操作？
   - 预期结果明确性: 预期结果是否具体、可验证？
3. 发现问题后，对需要修改的模块逐个调用 `save_testcases(append_module=修改后的单个模块对象)` 保存
   - **Review 阶段也使用 `append_module` 逐模块更新**，按模块名自动替换已有同名模块
   - 不需要修改的模块无需重新提交，避免全量提交导致参数截断
4. 重复步骤 1-3，迭代 2-3 轮
5. **在 Review 过程中，记录发现的需求疑问点和确认项**，用于最终报告

### 阶段5: 首次导出 + 用例概述
自动 Review 完成后：
1. 调用 `export_xmind` 导出 XMind 文件（自动命名为 `需求名_testCase.xmind`）
2. 调用 `export_report(questions=[...])` 导出测试报告（自动命名为 `需求名_testCaseReport.md`）
   - `questions` 参数传入 Review 过程中发现的需求疑问点列表
   - 报告包含：模块概览、覆盖维度统计、需求疑问点
3. **向用户输出用例生成概述**，内容包括：
   - 用例生成情况总结：共生成 X 个模块、X 个子模块、X 个用例
   - 各模块用例分布概览
   - 覆盖维度统计（正向/边界/异常/安全/性能）
   - 可能需要产品澄清或测试确认的疑问点列表
4. **询问用户提供 Kiro credit 消耗数据**：

> 请查看聊天窗口底部的 "Credits used" 和 "Elapsed time" 数值，告诉我具体数字，我会写入 JSON 报告。
> 如果不需要记录，直接说"跳过"即可。

5. 根据用户回复调用 `export_json_report` 导出 JSON 报告：
   - 用户提供了数字 → `export_json_report(agent_model="当前模型名", credits_used=数字, elapsed_time="时间")`
   - 用户说"跳过" → `export_json_report(agent_model="当前模型名")`
   - `agent_model` 参数传入当前 agent 使用的模型名称（从 system prompt 中的 model_information 获取）
6. 调用 `upload_to_cos` 将 JSON 报告上传到腾讯云 COS：
   - 参数使用用户预配置的 COS 信息（如在对话中提供）
   - 上传成功后向用户展示 COS URL
   - 上传失败时提示错误信息，不阻塞后续流程
7. **告知用户去查看 XMind 文件**，请用户自行确认用例是否完善

向用户发送的消息模板：

> 用例已生成并导出，请查看：
> - XMind 文件：`需求名_testCase.xmind`
> - 测试报告：`需求名_testCaseReport.md`
> - JSON 报告：`需求名_report.json`
>
> **生成概述：** 共 X 个模块、X 个用例，覆盖正向功能/边界条件/异常处理/安全性等维度。
>
> **需要确认的疑问点：**
> 1. xxx
> 2. xxx
>
> 请查看 XMind 文件中的用例，确认后告诉我：
> - **用例已完善** → 结束流程
> - **需要补充/修改** → 请描述需要调整的内容，我会更新用例

### 阶段6: 用户验收循环

进入用户验收循环，**等待用户反馈**：

#### 6.1 用户确认完善 → 结束流程
如果用户回复"完善"、"没问题"、"确认"等肯定回复：
- 输出最终确认信息，流程结束

#### 6.2 用户反馈需要补充/修改 → 迭代更新
如果用户提出修改意见（如遗漏场景、用例不准确、需要补充等）：
1. 根据用户反馈，定位需要修改的模块
2. 如需回顾文档内容，调用 `get_doc_section` 重新读取相关章节
3. 修改或补充对应模块的用例
4. 调用 `save_testcases(append_module=修改后的单个模块对象)` 保存更新
5. 调用 `export_xmind` 重新导出 XMind 文件（覆盖原文件）
6. 调用 `export_report` 重新导出测试报告（覆盖原文件）
7. 调用 `export_json_report` 重新导出 JSON 报告（覆盖原文件）
8. 调用 `upload_to_cos` 重新上传 JSON 报告到 COS（覆盖原文件）
9. **再次向用户输出本轮修改概述**，说明修改了哪些内容
10. **再次请用户确认**用例是否完善

**重复阶段 6，直到用户确认用例完善为止。**

## 与用户交互

- 启动时如有缓存任务，必须询问用户是继续还是重新开始
- 新任务开始前，必须确认文档已就位
- 新任务开始前，必须让用户选择处理模式（文档+图片 / 纯文档）
- 对文档内容有疑问时，主动向用户确认，不要自行猜测
- **自动 Review 完成后，必须先导出 XMind 和报告，然后向用户输出用例概述和疑问点，请用户查看 XMind 确认**
- **必须等待用户明确确认"用例完善"后才能结束流程，不能自行判断结束**
- **用户反馈需要修改时，修改用例后必须重新导出 XMind 和报告，再次请用户确认**

## 跨 session 恢复机制

所有状态自动持久化到 `.tmp/cache/` 目录：
- `phase_state.json` — 工作流阶段进度
- `image_progress.json` — 图片处理进度
- `testcases.json` — 已生成的测试用例
- `doc_summary.json` — 文档结构摘要

新 session 中调用 `setup_environment` 即可检测缓存任务，询问用户后决定恢复或重新开始。

## 注意事项
- 对文档中的专业术语保持原样，不要随意翻译或替换
- 表格中的字段约束应转化为具体的测试用例
- 状态机/流程图中的每条路径都应有对应的测试用例
- 接口文档中的每个参数都应有正向和反向测试
- 图片分析结果已直接写入 Markdown 文件，生成用例时注意上下文关联
- 接口的业务规则（如删除前校验关联数据、状态流转限制）是用例设计的核心依据
- 每个用例的预期结果必须包含具体的返回值或错误提示文案，不能用"操作失败"等模糊描述
- 前置条件必须明确数据状态，如"存在状态为0-Building的站点，已配置计费规则"
- 步骤中涉及接口调用时，需明确请求方式、路径和关键参数
- `save_testcases` 的 `append_module` 参数必须是单个模块对象（非数组），`modules` 参数必须是数组
- **始终优先使用 `append_module` 逐模块保存**，`modules` 全量替换仅在模块极少时使用，大量用例时会因参数过大导致截断失败

## 图片分析质量标准

### 合格的分析示例（数据表截图）：
```
| 字段名 | 类型 | 长度 | 必填 | 默认值 | 描述 |
|--------|------|------|------|--------|------|
| station_id | VARCHAR | 64 | 是 | - | 站点唯一标识 |
| station_name | VARCHAR | 128 | 是 | - | 站点名称 |
```

### 不合格的分析示例：
```
数据表结构定义截图 - 展示了数据库表的字段列表，包含字段名、数据类型等。
```
这种概括性描述对测试用例设计毫无帮助。
